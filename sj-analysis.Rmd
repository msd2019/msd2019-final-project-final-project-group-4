---
title: "Analysis"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

# Replicating Results in the Original Paper

```{r embedding-loop}
embedding_loop <- function(temp_test, em_level) {

  temp_test <- temp_test %>% filter(embeddedness >= em_level)
  
  mylogit <- glm(sign ~ embeddedness + pos_in.x + neg_in.y + total_out.x + total_in.y + pos_out.x + neg_out.y, data = temp_test, family = "binomial")
  summary(mylogit)
  
  temp_x <- temp_test[,-c(1,2,3)]
  temp_y <- temp_test[,3]
  cvfitted <- cv.glmnet(as.matrix(temp_x),temp_y, family="binomial")
  
  #plot(cvfitted)
  print(cvfitted$lambda.min)
  
  log_odds_predictions <-predict(cvfitted,as.matrix(temp_x), s="lambda.min")
  prob_predictions <- exp(log_odds_predictions)/(1 + exp(log_odds_predictions))
  prob_predictions_actual <- cbind(prob_predictions, temp_y)
  colnames(prob_predictions_actual) <- c("probability","actual")
  
  prob_predictions_actual <- as_tibble(prob_predictions_actual)
  prob_predictions_actual <- prob_predictions_actual %>%
    mutate(predicted_sign =  ifelse(probability < .5, 0, 1)) %>%
    mutate(match = ifelse(predicted_sign == actual, 1,0))
  
  print(mean(prob_predictions_actual$match))
  return(mean(prob_predictions_actual$match))
}
```

Old Wikipedia Data Logistic Regression

```{r old wikipedia logistic regression}
oldwiki_edges <- read.csv("data/oldwiki_edgelist.csv", col.names = c("u", "v", "sign"))
oldwiki_nodes <-read_csv("data/nodes_features_oldwiki.csv")
oldwiki_embed <-read_csv("data/edges_features_old_wiki.csv")

oldwiki_sign_embed <- inner_join(oldwiki_edges, oldwiki_embed, by = c('u'='u', 'v' = 'v'))
oldwiki_sign_embed <- inner_join(oldwiki_sign_embed, oldwiki_nodes, by = c('u' = 'node_id'))
oldwiki_sign_embed <- inner_join(oldwiki_sign_embed, oldwiki_nodes, by = c('v' = 'node_id'))
test <- select(oldwiki_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test <- test %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_oldwiki <- vector('numeric', 3)
pred_accuracy_oldwiki[1] <- embedding_loop(test, 0)
pred_accuracy_oldwiki[2] <- embedding_loop(test, 10)
pred_accuracy_oldwiki[3] <- embedding_loop(test, 25)
```


Slashdot Data Logistic Regression and Predictive Accuracy

```{r slashdot logistic regression}
slashdot_edges <- read.csv("data/edgelist_slashdot.csv", col.names = c("u", "v", "sign"))
slashdot_nodes <-read_csv("data/nodes_features_slashdot.csv")
slashdot_embed <-read_csv("data/edges_features_slashdot.csv")

slashdot_sign_embed <- inner_join(slashdot_edges, slashdot_embed, by = c('u'='u', 'v' = 'v'))
slashdot_sign_embed <- inner_join(slashdot_sign_embed, slashdot_nodes, by = c('u' = 'node_id'))
slashdot_sign_embed <- inner_join(slashdot_sign_embed, slashdot_nodes, by = c('v' = 'node_id'))
test_slashdot <- select(slashdot_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_slashdot <- test_slashdot %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_slashdot <- vector('numeric', 3)
pred_accuracy_slashdot[1] <- embedding_loop(test_slashdot, 0)
pred_accuracy_slashdot[2] <- embedding_loop(test_slashdot, 10)
pred_accuracy_slashdot[3] <- embedding_loop(test_slashdot, 25)
```

Epinions Data Logistic Regression and Predictive Accuracy

```{r epinions logistic regression}
epinions_edges <- read.csv("data/edgelist_epinions.csv", col.names = c("u", "v", "sign"))
epinions_nodes <-read_csv("data/nodes_features_epinions.csv")
epinions_embed <-read_csv("data/edges_features_epinions.csv")

epinions_sign_embed <- inner_join(epinions_edges, epinions_embed, by = c('u'='u', 'v' = 'v'))
epinions_sign_embed <- inner_join(epinions_sign_embed, epinions_nodes, by = c('u' = 'node_id'))
epinions_sign_embed <- inner_join(epinions_sign_embed, epinions_nodes, by = c('v' = 'node_id'))
test_epinions <- select(epinions_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_epinions <- test_epinions %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_epinions <- vector('numeric', 3)
pred_accuracy_epinions[1] <- embedding_loop(test_epinions, 0)
pred_accuracy_epinions[2] <- embedding_loop(test_epinions, 10)
pred_accuracy_epinions[3] <- embedding_loop(test_epinions, 25)
```

```{r predict accuracy bar graphs}
total_pred_acc <- cbind(c("0","10","25"), pred_accuracy_epinions, pred_accuracy_slashdot, pred_accuracy_oldwiki)

total_pred_acc <- as_tibble(total_pred_acc)
total_pred_acc <- rename(total_pred_acc, embeddedness_level = V1)

graph_epinions <- ggplot(data = total_pred_acc, aes(x = embeddedness_level, y = pred_accuracy_epinions)) + 
  geom_bar(stat = "identity") + 
  expand_limits(y=c(0.5, 1))

graph_slashdot <- ggplot(data = total_pred_acc, aes(x = embeddedness_level, y = pred_accuracy_slashdot)) + 
  geom_bar(stat = "identity") + 
  expand_limits(y=c(0.5, 1))

graph_oldwiki <- ggplot(data = total_pred_acc, aes(x = embeddedness_level, y = pred_accuracy_oldwiki)) + 
  geom_bar(stat = "identity") + 
  expand_limits(y=c(0.5, 1))

graph_epinions
graph_slashdot
graph_oldwiki
```

```{r cross prediction function}
cross_predict <- function(model, temp_test) {
  
  temp_x <- temp_test[,-c(1,2,3)]
  temp_y <- temp_test[,3]

  log_odds_predictions <-predict(model,as.matrix(temp_x), s="lambda.min")
  prob_predictions <- exp(log_odds_predictions)/(1 + exp(log_odds_predictions))
  prob_predictions_actual <- cbind(prob_predictions, temp_y)
  colnames(prob_predictions_actual) <- c("probability","actual")
  
  prob_predictions_actual <- as_tibble(prob_predictions_actual)
  prob_predictions_actual <- prob_predictions_actual %>%
    mutate(predicted_sign =  ifelse(probability < .5, 0, 1)) %>%
    mutate(match = ifelse(predicted_sign == actual, 1,0))
    
  cf <-confusionMatrix(table(prob_predictions_actual$predicted_sign,prob_predictions_actual$actual))
  print(as.table(cf))
  
  return(mean(prob_predictions_actual$match))
}
```

Train Logistic Regression Models

```{r train logistic regression models}
temp_x1 <- test[,-c(1,2,3)]
temp_x2 <- test_epinions[,-c(1,2,3)]
temp_x3 <- test_slashdot[,-c(1,2,3)]

temp_y1 <- test[,3]
temp_y2 <- test_epinions[,3]
temp_y3 <- test_slashdot[,3]

mylogit_wiki_old <- cv.glmnet(as.matrix(temp_x1),temp_y1, family = "binomial")
mylogit_epinions <- cv.glmnet(as.matrix(temp_x2),temp_y2, family = "binomial")
mylogit_slashdot <-cv.glmnet(as.matrix(temp_x3),temp_y3, family = "binomial")
```

Cross Predict 

```{r perform cross predictions}
cross_predict(mylogit_wiki_old,test_epinions)
cross_predict(mylogit_wiki_old,test_slashdot)

cross_predict(mylogit_slashdot,test)
cross_predict(mylogit_slashdot,test_epinions)

cross_predict(mylogit_epinions,test)
cross_predict(mylogit_epinions,test_slashdot)
```

Leskovec, Huttenlocher, and Kleinberg reported the following predictive accuracy results. The rows represent the dataset trained on and the columns represent the dataset tested on. The machine learning formulation utilized for these results relied on all 23 features, including the triads. We chose to focus on the 7 node specific features for our computational results. 

\begin{table}[]
\begin{tabular}{llll}
          & Epinions & Slashdot & Wikipedia \\
Epinions  & 0.9342   & 0.9289   & 0.7722    \\
Slashdot  & 0.9249   & 0.9351   & 0.7717    \\
Wikipedia & 0.9272   & 0.9260   & 0.8021   
\end{tabular}
\end{table}

Our results are presented below with a focus on the 7 node specific features. It's important to note that all models tests on the Epinions and Wikipedia datasets yield similar results to Leskovec, Huttenlocher, and Kleinberg's original paper. However, we see larger disparities between the original results and our results with models tested on Slashdot. This suggests that the triad features carry larger importance for the Slashdot dataset than the Epinions and Wikipedia datasets. 

\begin{table}[]
\begin{tabular}{llll}
          & Epinions & Slashdot & Wikipedia \\
Epinions  & 0.8991   & 0.8125   & 0.7956    \\
Slashdot  & 0.8916   & 0.8424   & 0.7870    \\
Wikipedia & 0.9249   & 0.8446   & 0.8549   
\end{tabular}
\end{table}

# Considering New Wikipedia Data

```{r old wikipedia logistic regression}
newwiki_edges <- read.csv("data/edgelist_newwiki.csv", col.names = c("u", "v", "sign"))
newwiki_nodes <-read_csv("data/nodes_features_newwiki.csv")
newwiki_embed <-read_csv("data/edges_features_new_wiki.csv")

newwiki_sign_embed <- inner_join(newwiki_edges, newwiki_embed, by = c('u'='u', 'v' = 'v'))
newwiki_sign_embed <- inner_join(newwiki_sign_embed, newwiki_nodes, by = c('u' = 'node_id'))
newwiki_sign_embed <- inner_join(newwiki_sign_embed, newwiki_nodes, by = c('v' = 'node_id'))

test_newwiki <- select(newwiki_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_newwiki <- test_newwiki %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_oldwiki <- vector('numeric', 3)
pred_accuracy_oldwiki[1] <- embedding_loop(test, 0)
pred_accuracy_oldwiki[2] <- embedding_loop(test, 10)
pred_accuracy_oldwiki[3] <- embedding_loop(test, 25)
```

```{r perform cross predictions}
temp_x4 <- test_newwiki[,-c(1,2,3)]
temp_y4 <- test_newwiki[,3]

mylogit_newwiki <-cv.glmnet(as.matrix(temp_x4),temp_y4, family = "binomial")

cross_predict(mylogit_wiki_old,test_newwiki)
cross_predict(mylogit_slashdot, test_newwiki)
cross_predict(mylogit_epinions, test_newwiki)

cross_predict(mylogit_newwiki, test)
cross_predict(mylogit_newwiki, test_slashdot)
cross_predict(mylogit_newwiki, test_epinions)
```

We expand our table with the new results. Importantly, our logistic regerssion models on previous Wikipedia data originally cited in the article and our more recent new Wikipedia data show dissimilaries in predictive accuracy over the original three datasets in questions. Whereas the old wikipedia regression model and new wikpedia regression model had the same predictive accuracy on slashdot, there were slight differencs on testing performance on the Epinions datasets and the old Wikipedia dataset. 

\begin{table}[]
\begin{tabular}{lllll}
              & Epinions & Slashdot & Wikipedia & New Wikipedia \\
Epinions      & 0.8991   & 0.8125   & 0.7956    & 0.8270        \\
Slashdot      & 0.8916   & 0.8424   & 0.7870    & 0.9105        \\
Wikipedia     & 0.9249   & 0.8446   & 0.8549    & 0.9046        \\
New Wikipedia & 0.8921   & 0.8447   & 0.9378    & 0.9378       
\end{tabular}
\end{table}

# Extending our Results with the Bitcoin OTC Trust Network

We conduct a similar analysis as before on the Bitcoin OTC dataset to determine if the trained model on old wikipedia data can yield similar predictive accuracy as the other signed networks. 

```{r bitcoin logistic regression}
bitcoin_edges <- read.csv("data/bitcoin_edgelist.csv", col.names = c("u", "v", "sign"))
bitcoin_nodes <-read_csv("data/nodes_features_bitcoin.csv")
bitcoin_embed <-read_csv("data/edges_features_bitcoin.csv")

bitcoin_sign_embed <- inner_join(bitcoin_edges, bitcoin_embed, by = c('u'='u', 'v' = 'v'))
bitcoin_sign_embed <- inner_join(bitcoin_sign_embed, bitcoin_nodes, by = c('u' = 'node_id'))
bitcoin_sign_embed <- inner_join(bitcoin_sign_embed, bitcoin_nodes, by = c('v' = 'node_id'))
test <- select(bitcoin_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test <- test %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_bitcoin <- vector('numeric', 3)
pred_accuracy_bitcoin[1] <- embedding_loop(test, 0)
pred_accuracy_bitcoin[2] <- embedding_loop(test, 10)
pred_accuracy_bitcoin[3] <- embedding_loop(test, 25)
```

```{r bitcoin predictive accuracy bar chart}
total_pred_acc <- cbind(total_pred_acc, pred_accuracy_bitcoin)

graph_bitcoin <- ggplot(data = total_pred_acc, aes(x = embeddedness_level, 
                                                   y = pred_accuracy_bitcoin)) + 
  geom_bar(stat = "identity") + 
  expand_limits(y=c(0.5, 1))
```
