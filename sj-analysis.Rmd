---
title: "Analysis"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

# Replicating Results in the Original Paper

```{r embedding loop, Warning = FALSE, Message = FALSE}
embedding_loop <- function(temp_test, em_level) {

  temp_test <- temp_test %>% filter(embeddedness >= em_level)
  
  mylogit <- glm(sign ~ embeddedness + pos_in.x + neg_in.y + total_out.x + total_in.y + pos_out.x + neg_out.y, data = temp_test, family = "binomial")
  summary(mylogit)
  
  temp_x <- temp_test[,-c(1,2,3)]
  temp_y <- temp_test[,3]
  cvfitted <- cv.glmnet(as.matrix(temp_x),temp_y, family="binomial")
  
  #plot(cvfitted)
  print(cvfitted$lambda.min)
  
  log_odds_predictions <-predict(cvfitted,as.matrix(temp_x), s="lambda.min")
  prob_predictions <- exp(log_odds_predictions)/(1 + exp(log_odds_predictions))
  prob_predictions_actual <- cbind(prob_predictions, temp_y)
  colnames(prob_predictions_actual) <- c("probability","actual")
  
  prob_predictions_actual <- as_tibble(prob_predictions_actual)
  prob_predictions_actual <- prob_predictions_actual %>%
    mutate(predicted_sign =  ifelse(probability < .5, 0, 1)) %>%
    mutate(match = ifelse(predicted_sign == actual, 1,0))
  
  print(mean(prob_predictions_actual$match))
  return(mean(prob_predictions_actual$match))
}
```

## Old Wikipedia Data Logistic Regression

```{r old wikipedia logistic regression, Warning = FALSE, Message = FALSE}
oldwiki_edges <- read.csv("data/oldwiki_edgelist.csv", col.names = c("u", "v", "sign"))
oldwiki_nodes <-read_csv("data/nodes_features_oldwiki.csv")
oldwiki_embed <-read_csv("data/edges_features_old_wiki.csv")

oldwiki_sign_embed <- inner_join(oldwiki_edges, oldwiki_embed, by = c('u'='u', 'v' = 'v'))
oldwiki_sign_embed <- inner_join(oldwiki_sign_embed, oldwiki_nodes, by = c('u' = 'node_id'))
oldwiki_sign_embed <- inner_join(oldwiki_sign_embed, oldwiki_nodes, by = c('v' = 'node_id'))
test <- select(oldwiki_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test <- test %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_oldwiki <- vector('numeric', 3)
pred_accuracy_oldwiki[1] <- embedding_loop(test, 0)
pred_accuracy_oldwiki[2] <- embedding_loop(test, 10)
pred_accuracy_oldwiki[3] <- embedding_loop(test, 25)
```

## Slashdot Data Logistic Regression and Predictive Accuracy

```{r slashdot logistic regression, Warning = FALSE, Message = FALSE}
slashdot_edges <- read.csv("data/edgelist_slashdot.csv", col.names = c("u", "v", "sign"))
slashdot_nodes <-read_csv("data/nodes_features_slashdot.csv")
slashdot_embed <-read_csv("data/edges_features_slashdot.csv")

slashdot_sign_embed <- inner_join(slashdot_edges, slashdot_embed, by = c('u'='u', 'v' = 'v'))
slashdot_sign_embed <- inner_join(slashdot_sign_embed, slashdot_nodes, by = c('u' = 'node_id'))
slashdot_sign_embed <- inner_join(slashdot_sign_embed, slashdot_nodes, by = c('v' = 'node_id'))
test_slashdot <- select(slashdot_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_slashdot <- test_slashdot %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_slashdot <- vector('numeric', 3)
pred_accuracy_slashdot[1] <- embedding_loop(test_slashdot, 0)
pred_accuracy_slashdot[2] <- embedding_loop(test_slashdot, 10)
pred_accuracy_slashdot[3] <- embedding_loop(test_slashdot, 25)
```

## Epinions Data Logistic Regression and Predictive Accuracy

```{r epinions logistic regression, Warning = FALSE, Message = FALSE}
epinions_edges <- read.csv("data/edgelist_epinions.csv", col.names = c("u", "v", "sign"))
epinions_nodes <-read_csv("data/nodes_features_epinions.csv")
epinions_embed <-read_csv("data/edges_features_epinions.csv")

epinions_sign_embed <- inner_join(epinions_edges, epinions_embed, by = c('u'='u', 'v' = 'v'))
epinions_sign_embed <- inner_join(epinions_sign_embed, epinions_nodes, by = c('u' = 'node_id'))
epinions_sign_embed <- inner_join(epinions_sign_embed, epinions_nodes, by = c('v' = 'node_id'))
test_epinions <- select(epinions_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_epinions <- test_epinions %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_epinions <- vector('numeric', 3)
pred_accuracy_epinions[1] <- embedding_loop(test_epinions, 0)
pred_accuracy_epinions[2] <- embedding_loop(test_epinions, 10)
pred_accuracy_epinions[3] <- embedding_loop(test_epinions, 25)
```

```{r predict accuracy bar graphs, Warning = FALSE, Message = FALSE}
pred_acc_chunk_ep <- cbind(c("0","10","25"), pred_accuracy_epinions, c("epinions", "epinions", "epinions"))
pred_acc_chunk_sl <- cbind(c("0","10","25"), pred_accuracy_slashdot, c("slashdot", "slashdot", "slashdot"))
pred_acc_chunk_ow <- cbind(c("0","10","25"), pred_accuracy_oldwiki, c("oldwiki", "oldwiki", "oldwiki"))
reformatted_pred_acc <- rbind(pred_acc_chunk_ep, pred_acc_chunk_sl, pred_acc_chunk_ow)
reformatted_pred_acc <- as_tibble(reformatted_pred_acc)

colnames(reformatted_pred_acc) <- c("embed", "predacc", "website")
reformatted_pred_acc <- reformatted_pred_acc %>%
  mutate(predacc = as.numeric(predacc))

ggplot(data = reformatted_pred_acc, aes(website, predacc)) +
  geom_bar(aes(fill = embed), position = "dodge", stat = "identity") + 
  coord_cartesian(ylim=c(.5, 1))
```

Here, we have graphed the accuracy for each data set, testing on the complete data using 10-fold cross-validation as is done in the paper for different embedding levels. The results we get are quite similar to those found in the paper, with minor variations. This is probably due to the difference in cleaning the data. 

```{r cross prediction function, Warning = FALSE, Message = FALSE}
cross_predict <- function(model, temp_test) {
  
  temp_x <- temp_test[,-c(1,2,3)]
  temp_y <- temp_test[,3]

  log_odds_predictions <-predict(model,as.matrix(temp_x), s="lambda.min")
  prob_predictions <- exp(log_odds_predictions)/(1 + exp(log_odds_predictions))
  prob_predictions_actual <- cbind(prob_predictions, temp_y)
  colnames(prob_predictions_actual) <- c("probability","actual")
  
  prob_predictions_actual <- as_tibble(prob_predictions_actual)
  prob_predictions_actual <- prob_predictions_actual %>%
    mutate(predicted_sign =  ifelse(probability < .5, 0, 1)) %>%
    mutate(match = ifelse(predicted_sign == actual, 1,0))
    
  cf <-confusionMatrix(table(prob_predictions_actual$predicted_sign,prob_predictions_actual$actual))
  print(as.table(cf))
  
  return(mean(prob_predictions_actual$match))
}
```

## Train Logistic Regression Models

```{r train logistic regression models, Warning = FALSE, Message = FALSE}
temp_x1 <- test[,-c(1,2,3)]
temp_x2 <- test_epinions[,-c(1,2,3)]
temp_x3 <- test_slashdot[,-c(1,2,3)]

temp_y1 <- test[,3]
temp_y2 <- test_epinions[,3]
temp_y3 <- test_slashdot[,3]

mylogit_wiki_old <- cv.glmnet(as.matrix(temp_x1),temp_y1, family = "binomial")
mylogit_epinions <- cv.glmnet(as.matrix(temp_x2),temp_y2, family = "binomial")
mylogit_slashdot <-cv.glmnet(as.matrix(temp_x3),temp_y3, family = "binomial")
```

## Cross Predict 

```{r perform cross predictions, Warning = FALSE, Message = FALSE}
cross_predict(mylogit_wiki_old,test_epinions)
cross_predict(mylogit_wiki_old,test_slashdot)

cross_predict(mylogit_slashdot,test)
cross_predict(mylogit_slashdot,test_epinions)

cross_predict(mylogit_epinions,test)
cross_predict(mylogit_epinions,test_slashdot)
```

Leskovec, Huttenlocher, and Kleinberg reported the following predictive accuracy results. The rows represent the dataset trained on and the columns represent the dataset tested on. The machine learning formulation utilized for these results relied on all 23 features, including the triads. We chose to focus on the 7 node specific features for our computational results. 

\begin{table}[]
\begin{tabular}{llll}
          & Epinions & Slashdot & Wikipedia \\
Epinions  & 0.9342   & 0.9289   & 0.7722    \\
Slashdot  & 0.9249   & 0.9351   & 0.7717    \\
Wikipedia & 0.9272   & 0.9260   & 0.8021   
\end{tabular}
\end{table}

Our results are presented below with a focus on the 7 node specific features. It's important to note that all models tests on the Epinions and Wikipedia datasets yield similar results to Leskovec, Huttenlocher, and Kleinberg's original paper. However, we see larger disparities between the original results and our results with models tested on Slashdot. This suggests that the triad features carry larger importance for the Slashdot dataset than the Epinions and Wikipedia datasets. 

\begin{table}[]
\begin{tabular}{llll}
          & Epinions & Slashdot & Wikipedia \\
Epinions  & 0.8991   & 0.8125   & 0.7956    \\
Slashdot  & 0.8916   & 0.8424   & 0.7870    \\
Wikipedia & 0.9249   & 0.8446   & 0.8549   
\end{tabular}
\end{table}

# Considering New Wikipedia Data

```{r new wikipedia logistic regression, Warning = FALSE, Message = FALSE}
newwiki_edges <- read.csv("data/edgelist_newwiki.csv", col.names = c("u", "v", "sign"))
newwiki_nodes <-read_csv("data/nodes_features_newwiki.csv")
newwiki_embed <-read_csv("data/edges_features_new_wiki.csv")

newwiki_sign_embed <- inner_join(newwiki_edges, newwiki_embed, by = c('u'='u', 'v' = 'v'))
newwiki_sign_embed <- inner_join(newwiki_sign_embed, newwiki_nodes, by = c('u' = 'node_id'))
newwiki_sign_embed <- inner_join(newwiki_sign_embed, newwiki_nodes, by = c('v' = 'node_id'))

test_newwiki <- select(newwiki_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

test_newwiki <- test_newwiki %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_newwiki <- vector('numeric', 3)
pred_accuracy_newwiki[1] <- embedding_loop(test, 0)
pred_accuracy_newwiki[2] <- embedding_loop(test, 10)
pred_accuracy_newwiki[3] <- embedding_loop(test, 25)
```

```{r perform cross predictions for new Wikipedia data, Warning = FALSE, Message = FALSE}
temp_x4 <- test_newwiki[,-c(1,2,3)]
temp_y4 <- test_newwiki[,3]

mylogit_newwiki <-cv.glmnet(as.matrix(temp_x4),temp_y4, family = "binomial")

cross_predict(mylogit_wiki_old,test_newwiki)
cross_predict(mylogit_slashdot, test_newwiki)
cross_predict(mylogit_epinions, test_newwiki)

cross_predict(mylogit_newwiki, test)
cross_predict(mylogit_newwiki, test_slashdot)
cross_predict(mylogit_newwiki, test_epinions)
```

We expand our table with the new results. Importantly, our logistic regerssion models on previous Wikipedia data originally cited in the article and our more recent new Wikipedia data show dissimilaries in predictive accuracy over the original three datasets in questions. Whereas the old wikipedia regression model and new wikpedia regression model had the same predictive accuracy on slashdot, there were slight differencs on testing performance on the Epinions datasets and the old Wikipedia dataset. 

\begin{table}[]
\begin{tabular}{lllll}
              & Epinions & Slashdot & Wikipedia & New Wikipedia \\
Epinions      & 0.8991   & 0.8125   & 0.7956    & 0.8270        \\
Slashdot      & 0.8916   & 0.8424   & 0.7870    & 0.9105        \\
Wikipedia     & 0.9249   & 0.8446   & 0.8549    & 0.9046        \\
New Wikipedia & 0.8921   & 0.8447   & 0.9378    & 0.9378       
\end{tabular}
\end{table}

# Extending our Results with the Bitcoin OTC Trust Network

We conduct a similar analysis as before on the Bitcoin OTC dataset to determine if the trained model on old wikipedia data can yield similar predictive accuracy as the other signed networks. 

```{r bitcoin logistic regression, Warning = FALSE, Message = FALSE}
bitcoin_edges <- read.csv("data/edgelist_bitcoin.csv", col.names = c("u", "v", "sign"))
bitcoin_nodes <-read_csv("data/nodes_features_bitcoin.csv")
bitcoin_embed <-read_csv("data/edges_features_bitcoin.csv")

bitcoin_sign_embed <- inner_join(bitcoin_edges, bitcoin_embed, by = c('u'='u', 'v' = 'v'))
bitcoin_sign_embed <- inner_join(bitcoin_sign_embed, bitcoin_nodes, by = c('u' = 'node_id'))
bitcoin_sign_embed <- inner_join(bitcoin_sign_embed, bitcoin_nodes, by = c('v' = 'node_id'))

test_bitcoin <- select(bitcoin_sign_embed, u, v, sign, embeddedness, pos_in.x, neg_in.y, total_out.x, total_in.y, pos_out.x, neg_out.y)

#test_bitcoin <- test_bitcoin %>% filter(sign != 0) %>% mutate(sign =  ifelse(sign == -1, 0, 1))

pred_accuracy_bitcoin <- vector('numeric', 3)
pred_accuracy_bitcoin[1] <- embedding_loop(test, 0)
pred_accuracy_bitcoin[2] <- embedding_loop(test, 10)
pred_accuracy_bitcoin[3] <- embedding_loop(test, 25)
```

```{r bitcoin predictive accuracy bar chart, Warning = FALSE, Message = FALSE}
pred_acc_chunk_nw <- cbind(c("0","10","25"), pred_accuracy_newwiki, c("newwiki", "newwiki", "newwiki"))
pred_acc_chunk_bc <- cbind(c("0","10","25"), pred_accuracy_bitcoin, c("bitcoin", "bitcoin", "bitcoin"))

reformatted_pred_acc <- rbind(pred_acc_chunk_ep, pred_acc_chunk_sl, pred_acc_chunk_ow, pred_acc_chunk_nw, pred_acc_chunk_bc)
reformatted_pred_acc <- as_tibble(reformatted_pred_acc)

colnames(reformatted_pred_acc) <- c("embed", "predacc", "website")

reformatted_pred_acc <- reformatted_pred_acc %>%
  mutate(predacc = as.numeric(predacc))

ggplot(data = reformatted_pred_acc, aes(website, predacc)) +
  geom_bar(aes(fill = embed), position = "dodge", stat = "identity") + 
  coord_cartesian(ylim=c(.7, 1))
```

```{r bitcoin cross predictions, Warning = FALSE, Message = FALSE}
temp_x5 <- test_bitcoin[,-c(1,2,3)]
temp_y5 <- test_bitcoin[,3]

mylogit_bitcoin <-cv.glmnet(as.matrix(temp_x5), temp_y5, family = "binomial")

cross_predict(mylogit_wiki_old,test_bitcoin)
cross_predict(mylogit_slashdot, test_bitcoin)
cross_predict(mylogit_epinions, test_bitcoin)
cross_predict(mylogit_newwiki, test_bitcoin)

cross_predict(mylogit_bitcoin, test)
cross_predict(mylogit_bitcoin, test_slashdot)
cross_predict(mylogit_bitcoin, test_epinions)
cross_predict(mylogit_bitcoin, test_newwiki)
```

We summarize our final table of results below. 

\begin{table}[]
\begin{tabular}{llllll}
              & Epinions & Slashdot & Wikipedia & New Wikipedia & Bitcoin \\
Epinions      & 0.8991   & 0.8125   & 0.7956    & 0.8270        & 0.7143  \\
Slashdot      & 0.8916   & 0.8424   & 0.7870    & 0.9105        & 0.5988  \\
Wikipedia     & 0.9249   & 0.8446   & 0.8549    & 0.9046        & 0.5592  \\
New Wikipedia & 0.8921   & 0.8447   & 0.9378    & 0.9378        & 0.3738  \\
Bitcoin       & 0.7262   & 0.7604   & 0.4826    & 0.4826        & 0.9378 
\end{tabular}
\end{table}

Interestingly, the Bitcoin datasets shows very little similarity to the Wikipedia, Slashdot, and Epinions datasets. In an initial analysis, we found that the average positivity of the graph does not seem to be a strong predictor on graphs' intra-predictive similarities. We see that the wikipedia datasets have average positivities approximately equal to 47% whereas slashdot and epinion both have average positivities approximately equalt to 77% while the bitcoin dataset is approximately 85%. 

```{r average positivity, Warning = FALSE, Message = FALSE}
mean(test$sign)
mean(test_epinions$sign)
mean(test_slashdot$sign)
mean(test_newwiki$sign)
mean(test_bitcoin$sign)
```

Earlier in our analysis we suggested that the triad features carried larger importance for the Slashdot dataset than the Epinions and Wikipedia datasets. We considered the embeddedness of each dataset and found surprising results. The embeddedness of slashdot and the bitcoin dataset were most similar at approximately 4.0 whereas the embeddedness of epinions and the wikipedia datasets were 24 and 14 respectively. The smaller average embeddedness suggests an inherently different network structure which yieled to our poor predictive accuracy. 

```{r embeddedness histograms, Warning = FALSE, Message = FALSE}
mean(test$embeddedness)
mean(test_epinions$embeddednes)
mean(test_slashdot$embeddednes)
mean(test_newwiki$embeddednes)
mean(test_bitcoin$embeddednes)
```