# Final Project for Modeling Social Data, 2019

SAMEER JAIN (sj2736) 
BAILEY PIERSON (bp2471) 
TANMAY CHOPRA (tc2897)

This repository has code to attempt to replicate and extend the results in [Predicting Positive and Negative Links in Online Social Networks](www.arxiv.org/abs/1003.2429) by Jure Leskovec, Daniel Huttenlocher, and Jone Kleinberg, WWW 2010: ACM WWW International conference on World Wide Web, 2010.

A complete report of our results is in `05_final_report.pdf`, which can be generated by cloning this repository and running `make` to execute the commands in the `Makefile` file. All data are in `data/`. Our group was not available to find original source code provided by the authors.

The repository is structured as follows:

1. `01_get_original_data.sh` gets the original data used by the authors and places a copy in `data/`
2. `02_clean_original_data.sh` cleans this data and saves the relevant dataframe(s) in `data/original_data_clean.Rdata`
3. `03_get_new_data.sh` gets new data used to extend the original results of this paper and places a copy in `data/`
4. `04_clean_new_data.sh` cleans this data and saves the relevant dataframe(s) in `data/new_data_clean.Rdata`
5. `05_final_report.Rmd` analysis both the original and new data to replicate and extend the results of the original paper, and produces the final report `05_final_report.pdf`

----

# Instructions

Running `make scripts` runs `01_get_original_data.sh`, `02_clean_original_data.sh`, `03_get_new_data.sh`, and `04_clean_new_data.sh`. This downloads the original data sets and removes unnecessary comments from. We then process the data with several python scripts. Each python scripts takes a substantive amount of time to run, because we specifically calculate the embeddedness of each node in each graph studied. For this reason, we ran our scripts locally and uploaded the resulting outputs as CSV files in the data directory. We process these CSV files in our R analysis. Readers can examine the original data after frunning `make scripts`. Running `make clean` deletes all downloaded data from the `data` subdirectory.  

Running `make all` should produce a rendered HTML file of our analysis, code, and results. Note that this process takes a very long time.

Downloading new Wikipedia data and generating the features for the machine learning model requires the following package installations: 

- Python NetworkX
- Python Pandas
- Python collections
- Python bs4 
- Python Requests
- Python lxml

Each of these packages can be installed via pip. We utilized Python 3 for new data collection and computation of triad features for the machine learning model. 
